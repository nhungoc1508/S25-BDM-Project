from pyspark.sql import SparkSession
from delta import *
from pyspark.sql.functions import col
import duckdb

MONGO_URI = 'mongodb://root:root@counseling-db:27017/?authSource=admin'
BASE_TRUSTED_PATH = '/data/trusted'
db_path = f'{BASE_TRUSTED_PATH}/databases/trusted_data.db'

def init_spark():
    builder = SparkSession.builder \
        .appName("CounselorsOrganizing") \
        .config("spark.driver.bindAddress", "0.0.0.0") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
    
    spark = configure_spark_with_delta_pip(builder).getOrCreate()
    return spark

def read_data(spark):
    df = spark.read.format("mongodb") \
        .option("spark.mongodb.read.connection.uri", MONGO_URI) \
        .option("database", "counseling") \
        .option("collection", "counselors") \
        .load()
    return df

def insert_relational(df):
    conn = duckdb.connect(db_path)
    part_df = df.select("counselor_id", col("personal_info.name.first").alias("first_name"), col("personal_info.name.last").alias("last_name"), "personal_info.name.title")
    pdf = part_df.toPandas()

    try:
        conn.execute("""
            CREATE TABLE IF NOT EXISTS expl_counselors (
                counselor_id VARCHAR PRIMARY KEY,
                first_name VARCHAR,
                last_name VARCHAR,
                title VARCHAR
            );
        """)
    except Exception as e:
        print("[ORGANIZING TASK] Failed to create table:", e)
    
    conn.execute('DELETE FROM expl_counselors;')
    conn.execute('INSERT INTO expl_counselors SELECT * FROM pdf;')
    print("[ORGANIZING TASK] After inserting:", conn.execute("SELECT COUNT(*) FROM expl_counselors;").fetchone())

if __name__ == '__main__':
    spark = init_spark()
    df = read_data(spark)
    insert_relational(df)
    spark.sparkContext.stop()
    print(f'[ORGANIZING TASK] Organizing completed for counselors data')