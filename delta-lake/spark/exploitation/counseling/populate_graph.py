from pyspark.sql import SparkSession
from delta import *
from pyspark.sql.functions import col
import duckdb

import os
from dotenv import load_dotenv
import json
import keybert
from neo4j import GraphDatabase
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from pinecone import Pinecone, ServerlessSpec

from utils.config import Config

def init_spark():
    builder = SparkSession.builder \
        .appName("GraphPopulation") \
        .config("spark.driver.bindAddress", "0.0.0.0") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
    
    spark = configure_spark_with_delta_pip(builder).getOrCreate()
    return spark

def read_data(spark):
    df_counselors = spark.read.format("mongodb") \
                    .option("spark.mongodb.read.connection.uri", Config.MONGO_URI) \
                    .option("database", Config.MONGO_DB) \
                    .option("collection", Config.COUNSELORS_COLLECTION) \
                    .load()
    
    df_requests = spark.read.format("mongodb") \
                  .option("spark.mongodb.read.connection.uri", Config.MONGO_URI) \
                  .option("database", Config.MONGO_DB) \
                  .option("collection", Config.REQUESTS_COLLECTION) \
                  .load()
    
    df_reports = spark.read.format("mongodb") \
                 .option("spark.mongodb.read.connection.uri", Config.MONGO_URI) \
                 .option("database", Config.MONGO_DB) \
                 .option("collection", Config.REPORTS_COLLECTION) \
                 .load()
    
    return df_counselors, df_requests, df_reports

def run_query(tx, query, parameters=None):
    tx.run(query, parameters or {})

def create_node(tx, label, props):
    query = f"""
    MERGE (n:{label} {{id: $id}})
    SET n += $props
    """
    tx.run(query, {'id': props['id'], 'props': props})

def create_relationship(tx, from_id, to_id, from_label, to_label, rel_type, rel_attrs=None):
    query = f"""
    MATCH (a:{from_label} {{id: $from_id}})
    MATCH (b:{to_label} {{id: $to_id}})
    MERGE (a)-[r:{rel_type}]->(b)
    """
    if rel_attrs:
        set_clauses = [f"r.{key} = ${key}" for key in rel_attrs.keys()]
        query += "\nSET " + ", ".join(set_clauses)
    parameters = {'from_id': from_id, 'to_id': to_id}
    if rel_attrs:
        parameters.update(rel_attrs)
    tx.run(query, parameters)

def extract_keywords(kw_model, text, top_n=5):
    return kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 2),
                                     stop_words='english', use_mmr=True,
                                     diversity=0.3, top_n=top_n)

def get_embedding(embedder, text):
    return embedder.encode([text])[0].tolist()

def find_similar_keyword(index, embedder, keyword, threshold=0.8):
    embedding = get_embedding(embedder, keyword)
    
    results = index.query(
        vector=embedding,
        top_k=1,
        include_metadata=True
    )
    
    if results['matches'] and results['matches'][0]['score'] >= threshold:
        return results['matches'][0]['metadata']['keyword']
    
    return None

def add_keyword(index, embedder, keyword):
    embedding = get_embedding(embedder, keyword)
    keyword_id = keyword.replace(' ', '_')
    keyword_id = ''.join([i if ord(i) < 128 else '' for i in keyword_id])
    index.upsert(vectors=[
            {
                'id': keyword_id,
                'values': embedding,
                'metadata': {'keyword': keyword}
            }
        ],
        namespace="__default__"
    )

def create_counselor_nodes(driver, counselors):
    with driver.session() as session:
        for counselor in counselors:
            session.execute_write(create_node, 'Counselor', {'id': counselor['id']})
            # Primary specialization
            primary_spec = counselor['primary_specialization'].lower().replace(' ', '_')
            session.execute_write(create_node, 'Specialization', {'id': primary_spec, 'label': counselor['primary_specialization']})
            session.execute_write(create_relationship, counselor['id'], primary_spec, 'Counselor', 'Specialization', 'HAS_PRIMARY_SPECIALIZATION')
            # Secondary specializations
            for spec in counselor['secondary_specializations']:
                secondary_spec = spec.lower().replace(' ', '_')
                session.execute_write(create_node, 'Specialization', {'id': secondary_spec, 'label': spec})
                session.execute_write(create_relationship, counselor['id'], secondary_spec, 'Counselor', 'Specialization', 'HAS_SECONDARY_SPECIALIZATION')
            # Academic disciplines
            for dept in counselor['academic_disciplines']:
                session.execute_write(create_node, 'Department', {'id': dept})
                session.execute_write(create_relationship, counselor['id'], dept, 'Counselor', 'Department', 'SUPPORTS_DISCIPLINE')
            # Student demographics
            for dept in counselor['student_demographics']:
                session.execute_write(create_node, 'Department', {'id': dept})
                session.execute_write(create_relationship, counselor['id'], dept, 'Counselor', 'Department', 'SUPPORTS_DISCIPLINE')
            # Expertise
            for keyword in counselor['expertise']:
                keyword_id = keyword.lower().replace(' ', '_')
                session.execute_write(create_node, 'Keyword', {'id': keyword_id, 'label': keyword})
                session.execute_write(create_relationship, counselor['id'], keyword_id, 'Counselor', 'Keyword', 'HAS_EXPERTISE')

def get_student_info(student_id):
    conn = duckdb.connect(Config.DB_PATH)
    # trusted_student_enrollment: student_id, academic_year, year, major
    # trusted_enrollment: student_id, course_code, semester, is_taking, grade
    query_background = f"SELECT * FROM trusted_student_enrollment WHERE student_id = '{student_id}'"
    query_enrollment = F"SELECT * FROM trusted_enrollment WHERE student_id = '{student_id}'"
    result_background = conn.execute(query_background).fetchone()
    result_enrollment = conn.execute(query_enrollment).fetchall()
    conn.close()
    obj = {
        'id': student_id,
        'year': result_background[2],
        'department': result_background[3],
        'courses': []
    }
    for res in result_enrollment:
        obj['courses'].append({
            'course_code': res[1],
            'is_taking': res[3],
            'grade': res[4]
        })
    return obj

def get_dept_of_course(course_code):
    # trusted_course: courde_code, title, description, dept_code, ...
    conn = duckdb.connect(Config.DB_PATH)
    query = f"SELECT * FROM trusted_course WHERE course_code = '{course_code}'"
    res = conn.execute(query).fetchone()
    return res[3]

def create_student_nodes(driver, students):
    with driver.session() as session:
        for _, student in students.items():
            session.execute_write(create_node, 'Student', {'id': student['id']})
            session.execute_write(create_node, 'Department', {'id': student['department']})
            session.execute_write(create_relationship, student['id'], student['department'], 'Student', 'Department', 'MAJORS_IN')
            for course in student['courses']:
                course_id = course['course_code'].lower().replace(' ', '_')
                session.execute_write(create_node, 'Course', {'id': course_id, 'course_code': course['course_code']})
                # course_attrs = {'is_taking': course['is_taking'], 'grade': course['grade']}
                course_attrs = {'is_taking': course['is_taking']}
                session.execute_write(create_relationship, student['id'], course_id, 'Student', 'Course', 'ENROLLED_IN', course_attrs)
                dept_code = get_dept_of_course(course['course_code'])
                if dept_code != None:
                    session.execute_write(create_node, 'Department', {'id': dept_code})
                    session.execute_write(create_relationship, course_id, dept_code, 'Course', 'Department', 'BELONGS_TO')

def create_request_nodes(driver, index, embedder, kw_model, requests):
    with driver.session() as session:
        for req in requests:
            session.execute_write(create_node, 'MeetingRequest', req)
            session.execute_write(create_relationship, req['student_id'], req['id'], 'Student', 'MeetingRequest', 'MADE_REQUEST')
            keywords = extract_keywords(kw_model, req['description'])
            for kw, _ in keywords:
                similar = find_similar_keyword(index, embedder, kw)
                if not similar:
                    kw_id = kw.lower().replace(' ', '_')
                else:
                    kw = similar
                    kw_id = similar.lower().replace(' ', '_')
                session.execute_write(create_node, 'Keyword', {'id': kw_id, 'label': kw})
                session.execute_write(create_relationship, req['id'], kw_id, 'MeetingRequest', 'Keyword', 'MENTIONS')

def create_report_nodes(driver, reports):
    with driver.session() as session:
        for report in reports:
            session.execute_write(create_node, 'MeetingReport', report)
            session.execute_write(create_relationship, report['counselor_id'], report['id'], 'Counselor', 'MeetingReport', 'HANDLED')
            session.execute_write(create_relationship, report['student_id'], report['id'], 'Student', 'MeetingReport', 'MEETING_WITH')
            session.execute_write(create_relationship, report['id'], report['request_id'], 'MeetingReport', 'MeetingRequest', 'REPORT_FOR')

def processing_pipeline(spark):
    load_dotenv()
    pinecone = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
    index = pinecone.Index(Config.INDEX_NAME)
    embedder = SentenceTransformer(Config.EMBEDDING_MODEL)
    kw_model = keybert.KeyBERT(model=Config.EMBEDDING_MODEL)
    driver = GraphDatabase.driver(Config.NEO4J_URI, auth=(Config.NEO4J_USERNAME, Config.NEO4J_PASSWORD))
    try:
        driver.verify_connectivity()
    except Exception as e:
        print('[ENRICH TASK] Failed to connect to Neo4J: {e}')
        return
    
    df_counselors, df_requests, df_reports = read_data(spark)
    print('[ENRICH TASK] Completed reading data')
    counselor_objs = []
    for row in df_counselors.toLocalIterator():
        obj = {
            'id': row['counselor_id'],
            'academic_disciplines': row['expertise']['academic_disciplines'],
            'primary_specialization': row['expertise']['primary_specialization'],
            'secondary_specializations': row['expertise']['secondary_specializations'],
            'expertise': [],
            'student_demographics': [ele['major_code'] for ele in row['performance_metrics']['student_demographics']['by_major_count']]
        }
        fields = ['issue_expertise', 'expertise_tags', 'expertise_weights_types']
        for field in fields:
            for exp in row['expertise'][field]:
                obj['expertise'].append(exp)
        for exp in row['performance_metrics']['intervention_statistics']['intervention_types']:
            obj['expertise'].append(exp)
        counselor_objs.append(obj)
    create_counselor_nodes(driver, counselor_objs)
    print('[ENRICH TASK] Completed creating counselor nodes')

    request_objs = []
    student_objs = dict()
    for row in df_requests.toLocalIterator():
        student_id = row['student_id']
        student_obj = get_student_info(student_id)
        obj = {
            'id': row['id'],
            'student_id': row['student_id'],
            'purpose': row['meeting_request']['purpose_lowercase_norm'],
            'description': row['meeting_request']['description_lowercase_norm']
        }
        request_objs.append(obj)
        if student_id not in student_objs:
            student_objs[student_id] = student_obj
    create_student_nodes(driver, student_objs)
    print('[ENRICH TASK] Completed creating student nodes')
    create_request_nodes(driver, index, embedder, kw_model, request_objs)
    print('[ENRICH TASK] Completed creating request nodes')

    report_objs = []
    for row in df_reports.toLocalIterator():
        report_objs.append({
            'id': row['id'],
            'request_id': row['request_id'],
            'student_id': row['student_id'],
            'counselor_id': row['counselor_id'],
            'full_text': row['full_text']
        })
    create_report_nodes(driver, report_objs)
    print('[ENRICH TASK] Completed creating report nodes')

if __name__ == '__main__':
    spark = init_spark()
    processing_pipeline(spark)
    spark.sparkContext.stop()
    print(f'[ENRICH TASK] Completed populating graph')