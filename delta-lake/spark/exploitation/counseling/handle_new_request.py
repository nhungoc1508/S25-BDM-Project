from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.types import StructType
from pyspark.sql.functions import col, struct, array, lower, to_timestamp
from pyspark.sql.functions import current_timestamp, current_date, lit, col
from delta import *
import duckdb
import numpy as np
from datetime import datetime
import time
import ftfy
import unicodedata
import re
import os
from dotenv import load_dotenv
import json
import keybert
from neo4j import GraphDatabase
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics.pairwise import cosine_similarity
from pinecone import Pinecone, ServerlessSpec

from utils.config import Config

from pymongo import MongoClient, UpdateOne

CHECKPOINTS_PATH = 'file:///data/tmp/checkpoints'
REQUESTS_DATA_PATH = 'file:///data/tmp/new_meeting_requests'

MONGO_URI = 'mongodb://root:root@counseling-db:27017/counseling?authSource=admin'
mongo_client = MongoClient(MONGO_URI)
db = mongo_client['counseling']
requests_collection = db['new-meeting-requests']

load_dotenv()
pinecone = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
index = pinecone.Index(Config.INDEX_NAME)
embedder = SentenceTransformer(Config.EMBEDDING_MODEL)
kw_model = keybert.KeyBERT(model=Config.EMBEDDING_MODEL)
driver = GraphDatabase.driver(Config.NEO4J_URI, auth=(Config.NEO4J_USERNAME, Config.NEO4J_PASSWORD))
try:
    driver.verify_connectivity()
except Exception as e:
    print('[ENRICH TASK] Failed to connect to Neo4J: {e}')

def normalize_text(text):
    if not isinstance(text, str):
        return text
    # Fix mojibake, HTML entities, and broken unicode
    text = ftfy.fix_text(text)
    text = unicodedata.normalize('NFKC', text)
    # Remove control/invisible characters
    text = re.sub(r'[\u0000-\u001F\u200b-\u200f\u202a-\u202e\u2060-\u206f]', '', text)
    return text

def lower_request(df):
    """
    Cast meeting_request.purpose/description to lowercase
    """
    df = df.withColumn(
        "meeting_request",
        struct(
            col("meeting_request.purpose"),
            col("meeting_request.description"),
            lower(col("meeting_request.purpose")).alias("purpose_lowercase"),
            lower(col("meeting_request.description")).alias("description_lowercase")
        )
    )
    return df

def normalize_request(df):
    """
    Normalize meeting_request.purpose/description to remove non-standard characters
    """
    df = df.withColumn(
        "meeting_request",
        struct(
            col("meeting_request.purpose"),
            col("meeting_request.description"),
            normalize_text(col("meeting_request.purpose_lowercase")).alias("purpose_lowercase_norm"),
            normalize_text(col("meeting_request.description_lowercase")).alias("description_lowercase_norm")
        )
    )
    return df

def cast_timestamp(df):
    """
    Cast timestamp to timestamp type
    """
    df = df.withColumn("timestamp", to_timestamp(col("timestamp")))
    return df

def preprocessing_pipeline(df):
    df = lower_request(df)
    df = normalize_request(df)
    df = cast_timestamp(df)
    return df

def run_query(tx, query, parameters=None):
    tx.run(query, parameters or {})

def create_node(tx, label, props):
    query = f"""
    MERGE (n:{label} {{id: $id}})
    SET n += $props
    """
    tx.run(query, {'id': props['id'], 'props': props})

def create_relationship(tx, from_id, to_id, from_label, to_label, rel_type, rel_attrs=None):
    query = f"""
    MATCH (a:{from_label} {{id: $from_id}})
    MATCH (b:{to_label} {{id: $to_id}})
    MERGE (a)-[r:{rel_type}]->(b)
    """
    if rel_attrs:
        set_clauses = [f"r.{key} = ${key}" for key in rel_attrs.keys()]
        query += "\nSET " + ", ".join(set_clauses)
    parameters = {'from_id': from_id, 'to_id': to_id}
    if rel_attrs:
        parameters.update(rel_attrs)
    tx.run(query, parameters)

def extract_keywords(text, top_n=5):
    return kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 2),
                                     stop_words='english', use_mmr=True,
                                     diversity=0.3, top_n=top_n)

def get_embedding(text):
    return embedder.encode([text])[0].tolist()

def find_similar_keyword(keyword, threshold=0.8):
    embedding = get_embedding(keyword)
    
    results = index.query(
        vector=embedding,
        top_k=1,
        include_metadata=True
    )
    
    if results['matches'] and results['matches'][0]['score'] >= threshold:
        return results['matches'][0]['metadata']['keyword']
    
    return None

def add_keyword(index, embedder, keyword):
    embedding = get_embedding(keyword)
    keyword_id = keyword.replace(' ', '_')
    keyword_id = ''.join([i if ord(i) < 128 else '' for i in keyword_id])
    index.upsert(vectors=[
            {
                'id': keyword_id,
                'values': embedding,
                'metadata': {'keyword': keyword}
            }
        ],
        namespace="__default__"
    )

def get_student_info(student_id):
    conn = duckdb.connect(Config.DB_PATH)
    # trusted_student_enrollment: student_id, academic_year, year, major
    # trusted_enrollment: student_id, course_code, semester, is_taking, grade
    query_background = f"SELECT * FROM trusted_student_enrollment WHERE student_id = '{student_id}'"
    query_enrollment = F"SELECT * FROM trusted_enrollment WHERE student_id = '{student_id}'"
    result_background = conn.execute(query_background).fetchone()
    result_enrollment = conn.execute(query_enrollment).fetchall()
    conn.close()
    obj = {
        'id': student_id,
        'year': result_background[2],
        'department': result_background[3],
        'courses': []
    }
    for res in result_enrollment:
        obj['courses'].append({
            'course_code': res[1],
            'is_taking': res[3],
            'grade': res[4]
        })
    return obj

def get_dept_of_course(course_code):
    # trusted_course: courde_code, title, description, dept_code, ...
    conn = duckdb.connect(Config.DB_PATH)
    query = f"SELECT * FROM trusted_course WHERE course_code = '{course_code}'"
    res = conn.execute(query).fetchone()
    return res[3]

def create_student_nodes(driver, students):
    print('[TASK] Creating student node')
    with driver.session() as session:
        for _, student in students.items():
            session.execute_write(create_node, 'Student', {'id': student['id']})
            session.execute_write(create_node, 'Department', {'id': student['department']})
            session.execute_write(create_relationship, student['id'], student['department'], 'Student', 'Department', 'MAJORS_IN')
            for course in student['courses']:
                course_id = course['course_code'].lower().replace(' ', '_')
                session.execute_write(create_node, 'Course', {'id': course_id, 'course_code': course['course_code']})
                # course_attrs = {'is_taking': course['is_taking'], 'grade': course['grade']}
                course_attrs = {'is_taking': course['is_taking']}
                session.execute_write(create_relationship, student['id'], course_id, 'Student', 'Course', 'ENROLLED_IN', course_attrs)
                dept_code = get_dept_of_course(course['course_code'])
                if dept_code != None:
                    session.execute_write(create_node, 'Department', {'id': dept_code})
                    session.execute_write(create_relationship, course_id, dept_code, 'Course', 'Department', 'BELONGS_TO')

def create_request_nodes(driver, index, embedder, kw_model, requests):
    print('[TASK] Creating request node')
    with driver.session() as session:
        for req in requests:
            session.execute_write(create_node, 'MeetingRequest', req)
            session.execute_write(create_relationship, req['student_id'], req['id'], 'Student', 'MeetingRequest', 'MADE_REQUEST')
            keywords = extract_keywords(req['description'])
            for kw, _ in keywords:
                similar = find_similar_keyword(kw)
                if not similar:
                    kw_id = kw.lower().replace(' ', '_')
                else:
                    kw = similar
                    kw_id = similar.lower().replace(' ', '_')
                session.execute_write(create_node, 'Keyword', {'id': kw_id, 'label': kw})
                session.execute_write(create_relationship, req['id'], kw_id, 'MeetingRequest', 'Keyword', 'MENTIONS')

def run_query(driver, query):
    with driver.session() as session:
        response = None
        try:
            response = session.run(query)
        except Exception as e:
            print(f'Failed to query: {query}\nError: {e}')
        session.close()
        return response

def create_gds_graph(driver):
    print('[TASK] Dropping old GDS graph')
    query = "CALL gds.graph.drop('counseling_graph', false) YIELD graphName;"
    run_query(driver, query)

    print('[TASK] Creating new GDS graph')
    query = """
    CALL gds.graph.project(
        'counseling_graph',
        ['Counselor', 'Course', 'Department', 'Keyword', 'MeetingReport', 'MeetingRequest', 'Specialization', 'Student'],
        {
            BELONGS_TO: {type: 'BELONGS_TO', orientation: 'UNDIRECTED'},
            ENROLLED_IN: {type: 'ENROLLED_IN', orientation: 'UNDIRECTED'},
            HANDLED: {type: 'HANDLED', orientation: 'UNDIRECTED'},
            HAS_EXPERTISE: {type: 'HAS_EXPERTISE', orientation: 'UNDIRECTED'},
            HAS_PRIMARY_SPECIALIZATION: {type: 'HAS_PRIMARY_SPECIALIZATION', orientation: 'UNDIRECTED'},
            HAS_SECONDARY_SPECIALIZATION: {type: 'HAS_SECONDARY_SPECIALIZATION', orientation: 'UNDIRECTED'},
            MADE_REQUEST: {type: 'MADE_REQUEST', orientation: 'UNDIRECTED'},
            MAJORS_IN: {type: 'MAJORS_IN', orientation: 'UNDIRECTED'},
            MEETING_WITH: {type: 'MEETING_WITH', orientation: 'UNDIRECTED'},
            MENTIONS: {type: 'MENTIONS', orientation: 'UNDIRECTED'},
            REPORT_FOR: {type: 'REPORT_FOR', orientation: 'UNDIRECTED'},
            SUPPORTS_DISCIPLINE: {type: 'SUPPORTS_DISCIPLINE', orientation: 'UNDIRECTED'}
        }
    );
    """
    return run_query(driver, query)

def embed(driver):
    query = """
    CALL gds.node2vec.stream(
        'counseling_graph',
        {
            embeddingDimension: 64,
            walkLength: 80,
            iterations: 20,
            returnFactor: 1.0,
            inOutFactor: 2.0
        }
    ) YIELD nodeId, embedding
    WITH gds.util.asNode(nodeId) AS node, embedding
    SET node.embedding = embedding
    """
    return run_query(driver, query)

def recommend_counselor(driver, request_id, top_n=3):
    with driver.session() as session:
        request_emb = session.run(
            """
            MATCH (r:MeetingRequest {id: $request_id})
            RETURN r.embedding AS emb
            """,
            request_id=request_id
        ).single()["emb"]
        results = session.run(
            """
            MATCH (c:Counselor)
            RETURN c.id AS id, c.embedding AS emb
            """
        )
        counselors = []
        for record in results:
            emb = np.array(record["emb"])
            sim = cosine_similarity([request_emb], [emb])[0][0]
            counselors.append((record["id"], sim))
        counselors.sort(key=lambda x: x[1], reverse=True)
        return counselors[:top_n]

def init_spark():
    builder = SparkSession.builder \
        .appName("RequestsReportsEmbedding") \
        .config("spark.driver.bindAddress", "0.0.0.0") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
    
    spark = configure_spark_with_delta_pip(builder).getOrCreate()
    return spark

def define_request_schema():
    return StructType([
        StructField("id", StringType(), True),
        StructField("student_id", StringType(), True),
        StructField("counselor_id", StringType(), True),
        StructField("meeting_request", StructType([
            StructField("purpose", StringType(), True),
            StructField("description", StringType(), True)
        ]), True),
        StructField("timestamp", StringType(), True)
    ])

def process_meeting_request_batch(df, batch_id):
    print(f"===== Batch ID: {batch_id} ======")
    print(f"Time: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    requests = df.collect()
    print(f"Processing {len(requests)} request(s)")
    df.show()
    
    # Preprocess
    df = preprocessing_pipeline(df)
    process_timestamp = datetime.now()
    process_date = process_timestamp.strftime("%Y-%m-%d")
    process_time = process_timestamp.strftime("%H-%M-%S")
    df = df.withColumn("_process_timestamp", lit(process_timestamp.isoformat())) \
           .withColumn("_process_date", lit(process_date))
    print('[TASK] Finished basic processing')

    # Upsert into Neo4J and get recommendation
    request_objs = []
    student_objs = dict()
    for row in df.toLocalIterator():
        student_id = row['student_id']
        student_obj = get_student_info(student_id)
        obj = {
            'id': row['id'],
            'student_id': row['student_id'],
            'purpose': row['meeting_request']['purpose_lowercase_norm'],
            'description': row['meeting_request']['description_lowercase_norm']
        }
        request_objs.append(obj)
        if student_id not in student_objs:
            student_objs[student_id] = student_obj
    create_student_nodes(driver, student_objs)
    create_request_nodes(driver, index, embedder, kw_model, request_objs)

    print('[TASK] Re-embedding the graph')
    res = create_gds_graph(driver)
    res = embed(driver)
    recs = recommend_counselor(driver, request_objs[0]['id'])[0]
    print(f'[TASK] Getting recommended counselor: {recs}')
    df = df.withColumn("recommended_counselor", lit(recs[0]))

    # Upsert into MongoDB
    clean_requests = [json.loads(row) for row in df.toJSON().collect()]
    operations = [
        UpdateOne(
            {'id': record['id']},
            {'$set': record},
            upsert=True
        )
        for record in clean_requests
    ]

    result = requests_collection.bulk_write(operations)
    print(f'[TASK] Finished upserting to MongoDB')
        
    print(f"Batch {batch_id} completed. Waiting for next batch...\n")
    df.show()
    
    time.sleep(30)

def pipeline(spark):
    streaming_df = spark.readStream \
                        .schema(define_request_schema()) \
                        .option("maxFilesPerTrigger", 1) \
                        .option("latestFirst", "false") \
                        .json(f"{REQUESTS_DATA_PATH}/*.json")
    
    query = streaming_df.writeStream \
                        .foreachBatch(process_meeting_request_batch) \
                        .outputMode("append") \
                        .option("checkpointLocation", CHECKPOINTS_PATH) \
                        .trigger(processingTime='30 seconds') \
                        .start()
    
    query.awaitTermination()

if __name__ == '__main__':
    spark = init_spark()
    pipeline(spark)