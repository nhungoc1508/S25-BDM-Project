from pyspark.sql import SparkSession
from delta import *
from pyspark.sql.functions import col
import duckdb

import numpy as np
import os
from dotenv import load_dotenv
import json
import keybert
from neo4j import GraphDatabase
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

from utils.config import Config

def init_spark():
    builder = SparkSession.builder \
        .appName("GraphEmbedding") \
        .config("spark.driver.bindAddress", "0.0.0.0") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
    
    spark = configure_spark_with_delta_pip(builder).getOrCreate()
    return spark

def run_query(driver, query):
    with driver.session() as session:
        response = None
        try:
            response = session.run(query)
        except Exception as e:
            print(f'Failed to query: {query}\nError: {e}')
        session.close()
        return response

def create_gds_graph(driver):
    query = """
    CALL gds.graph.project(
        'counseling_graph',
        ['Counselor', 'Course', 'Department', 'Keyword', 'MeetingReport', 'MeetingRequest', 'Specialization', 'Student'],
        {
            BELONGS_TO: {type: 'BELONGS_TO', orientation: 'UNDIRECTED'},
            ENROLLED_IN: {type: 'ENROLLED_IN', orientation: 'UNDIRECTED'},
            HANDLED: {type: 'HANDLED', orientation: 'UNDIRECTED'},
            HAS_EXPERTISE: {type: 'HAS_EXPERTISE', orientation: 'UNDIRECTED'},
            HAS_PRIMARY_SPECIALIZATION: {type: 'HAS_PRIMARY_SPECIALIZATION', orientation: 'UNDIRECTED'},
            HAS_SECONDARY_SPECIALIZATION: {type: 'HAS_SECONDARY_SPECIALIZATION', orientation: 'UNDIRECTED'},
            MADE_REQUEST: {type: 'MADE_REQUEST', orientation: 'UNDIRECTED'},
            MAJORS_IN: {type: 'MAJORS_IN', orientation: 'UNDIRECTED'},
            MEETING_WITH: {type: 'MEETING_WITH', orientation: 'UNDIRECTED'},
            MENTIONS: {type: 'MENTIONS', orientation: 'UNDIRECTED'},
            REPORT_FOR: {type: 'REPORT_FOR', orientation: 'UNDIRECTED'},
            SUPPORTS_DISCIPLINE: {type: 'SUPPORTS_DISCIPLINE', orientation: 'UNDIRECTED'}
        }
    );
    """
    return run_query(driver, query)

def embed(driver):
    query = """
    CALL gds.node2vec.stream(
        'counseling_graph',
        {
            embeddingDimension: 64,
            walkLength: 80,
            iterations: 20,
            returnFactor: 1.0,
            inOutFactor: 2.0
        }
    ) YIELD nodeId, embedding
    WITH gds.util.asNode(nodeId) AS node, embedding
    SET node.embedding = embedding
    """
    return run_query(driver, query)

def recommend_counselor(driver, request_id, top_n=3):
    with driver.session() as session:
        request_emb = session.run(
            """
            MATCH (r:MeetingRequest {id: $request_id})
            RETURN r.embedding AS emb
            """,
            request_id=request_id
        ).single()["emb"]
        results = session.run(
            """
            MATCH (c:Counselor)
            RETURN c.id AS id, c.embedding AS emb
            """
        )
        counselors = []
        for record in results:
            emb = np.array(record["emb"])
            sim = cosine_similarity([request_emb], [emb])[0][0]
            counselors.append((record["id"], sim))
        counselors.sort(key=lambda x: x[1], reverse=True)
        return counselors[:top_n]

def processing_pipeline(spark):
    embedder = SentenceTransformer(Config.EMBEDDING_MODEL)
    kw_model = keybert.KeyBERT(model=Config.EMBEDDING_MODEL)
    driver = GraphDatabase.driver(Config.NEO4J_URI, auth=(Config.NEO4J_USERNAME, Config.NEO4J_PASSWORD))
    try:
        driver.verify_connectivity()
    except Exception as e:
        print('[ENRICH TASK] Failed to connect to Neo4J: {e}')
        return

    res = create_gds_graph(driver)
    res = embed(driver)

if __name__ == '__main__':
    spark = init_spark()
    processing_pipeline(spark)
    spark.sparkContext.stop()
    print(f'[ENRICH TASK] Completed embedding graph')