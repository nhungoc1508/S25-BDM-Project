from pyspark.sql import SparkSession
from delta import *

import os
from dotenv import load_dotenv
import json
import keybert
from neo4j import GraphDatabase
from sentence_transformers import SentenceTransformer
from pinecone import Pinecone, ServerlessSpec

from utils.config import Config

MAPPING_PATH = 'file:///data/exploitation/counseling'

def init_spark():
    builder = SparkSession.builder \
        .appName("RequestsReportsEmbedding") \
        .config("spark.driver.bindAddress", "0.0.0.0") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
    
    spark = configure_spark_with_delta_pip(builder).getOrCreate()
    return spark

def read_data(spark):
    df_counselors = spark.read.format("mongodb") \
                    .option("spark.mongodb.read.connection.uri", Config.MONGO_URI) \
                    .option("database", Config.MONGO_DB) \
                    .option("collection", Config.COUNSELORS_COLLECTION) \
                    .load()
    
    df_requests = spark.read.format("mongodb") \
                  .option("spark.mongodb.read.connection.uri", Config.MONGO_URI) \
                  .option("database", Config.MONGO_DB) \
                  .option("collection", Config.REQUESTS_COLLECTION) \
                  .load()
    
    df_reports = spark.read.format("mongodb") \
                 .option("spark.mongodb.read.connection.uri", Config.MONGO_URI) \
                 .option("database", Config.MONGO_DB) \
                 .option("collection", Config.REPORTS_COLLECTION) \
                 .load()
    
    return df_counselors, df_requests, df_reports

def init_pinecone():
    pinecone = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
    if not pinecone.has_index(Config.INDEX_NAME):
        pinecone.create_index(
            name=Config.INDEX_NAME,
            vector_type='dense',
            dimension=Config.DIMENSION,
            metric='cosine',
            spec=ServerlessSpec(cloud='aws', region='us-east-1'),
            deletion_protection='disabled',
            tags={'environment': 'development'}
        )
    return pinecone

def get_embedding(embedder, text):
    return embedder.encode([text])[0].tolist()

def find_similar_keyword(index, embedder, keyword, threshold=0.8):
    embedding = get_embedding(embedder, keyword)
    
    results = index.query(
        vector=embedding,
        top_k=1,
        include_metadata=True
    )
    
    if results['matches'] and results['matches'][0]['score'] >= threshold:
        return results['matches'][0]['metadata']['keyword']
    
    return None

def add_keyword(index, embedder, keyword):
    embedding = get_embedding(embedder, keyword)
    keyword_id = keyword.replace(' ', '_')
    keyword_id = ''.join([i if ord(i) < 128 else '' for i in keyword_id])
    index.upsert(vectors=[
            {
                'id': keyword_id,
                'values': embedding,
                'metadata': {'keyword': keyword}
            }
        ],
        namespace="__default__"
    )

def process_counselor_keywords(index, embedder, objs):
    # primary & secondary specs, issue_expertise, expertise_tags, expertise_weights_types, intervention_types
    results = []
    for obj in objs:
        for field in obj.values():
            if not isinstance(field, list):
                field = [field]
            for kw in field:
                similar = find_similar_keyword(index, embedder, kw)
                results.append((kw, similar))
                if not similar:
                    add_keyword(index, embedder, kw)
    return results

def process_full_text(index, embedder, kw_model, texts):
    results = []
    for text in texts:
        keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 2),
                                             stop_words='english', use_mmr=True,
                                             diversity=0.3, top_n=5)
        for kw, _ in keywords:
            similar = find_similar_keyword(index, embedder, kw)
            results.append((kw, similar))
            if not similar:
                add_keyword(index, embedder, kw)
    return results

def processing_pipeline(spark):
    load_dotenv()
    pinecone = init_pinecone()
    index = pinecone.Index(Config.INDEX_NAME)
    embedder = SentenceTransformer(Config.EMBEDDING_MODEL)
    kw_model = keybert.KeyBERT(model=Config.EMBEDDING_MODEL)
    
    df_counselors, df_requests, df_reports = read_data(spark)
    print(f'[EMBEDDING TASK] Data read successfully, counselors={df_counselors.count()}, requests={df_requests.count()}, reports={df_reports.count()}')

    # Process counselors
    df_counselors_short = df_counselors.select('expertise.primary_specialization', 'expertise.secondary_specializations', 'expertise.issue_expertise', 'expertise.expertise_tags', 'expertise.expertise_weights_types', 'performance_metrics.intervention_statistics.intervention_types')
    counselor_objs = df_counselors_short.toPandas().to_dict(orient='records')
    counselor_res = process_counselor_keywords(index, embedder, counselor_objs)
    print('[EMBEDDING TASK] Finished embedding counselor data')

    # Process full texts
    req_texts = df_requests.select('meeting_request.description_lowercase_norm').rdd.map(lambda row: row[0]).collect()
    req_res = process_full_text(index, embedder, kw_model, req_texts)
    print('[EMBEDDING TASK] Finished embedding requests data')
    # rep_texts = df_reports.select('full_text').rdd.map(lambda row: row[0]).collect()
    # rep_res = process_full_text(index, embedder, kw_model, rep_texts)
    # print('[EMBEDDING TASK] Finished embedding reports data')

    # Save mappings
    schema = StructType([
        StructField("original_keyword", StringType(), True),
        StructField("similar_keyword", StringType(), True)
    ])
    keyword_mappings = counselor_res + req_res
    df_mappings = spark.createDataFrame(keyword_mappings, schema=schema)
    df_mappings.write.mode("overwrite") \
                     .format("delta") \
                     .save(f'{MAPPING_PATH}/keywords_mapping')
    print('[EMBEDDING TASKS] Finished saving mappings')
    return len(keyword_mappings)

if __name__ == '__main__':
    spark = init_spark()
    count = processing_pipeline(spark)
    spark.sparkContext.stop()
    print(f'[EMBEDDING TASK] Completed embedding counseling data, embedded {count} keywords')