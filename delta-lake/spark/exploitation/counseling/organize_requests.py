from pyspark.sql import SparkSession
from delta import *
from pyspark.sql.functions import col, to_timestamp, year, month, dayofmonth, hour, minute, broadcast
import duckdb

MONGO_URI = 'mongodb://root:root@counseling-db:27017/?authSource=admin'
BASE_TRUSTED_PATH = '/data/trusted'
db_path = f'{BASE_TRUSTED_PATH}/databases/trusted_data.db'

def init_spark():
    builder = SparkSession.builder \
        .appName("RequestsOrganizing") \
        .config("spark.driver.bindAddress", "0.0.0.0") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
    
    spark = configure_spark_with_delta_pip(builder).getOrCreate()
    return spark

def read_data(spark):
    df = spark.read.format("mongodb") \
        .option("spark.mongodb.read.connection.uri", MONGO_URI) \
        .option("database", "counseling") \
        .option("collection", "meeting-requests") \
        .load()
    return df

def generate_db_table(spark, df, conn):
    # Enrich timestamp data
    df = df.select("id", "student_id", "counselor_id", "timestamp")
    df = df.withColumn("timestamp", to_timestamp("timestamp", "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"))
    df = df.withColumn("year", year("timestamp")) \
           .withColumn("month", month("timestamp")) \
           .withColumn("day", dayofmonth("timestamp")) \
           .withColumn("hour", hour("timestamp"))

    # Filter by valid student IDs
    student_ids = conn.execute("SELECT student_id FROM trusted_student").fetchall()
    student_ids = [id[0] for id in student_ids]

    student_ids_df = spark.createDataFrame([(sid,) for sid in student_ids], ["student_id"])
    filtered_df = df.join(broadcast(student_ids_df), on="student_id", how="inner")
    print(f"[ORGANIZING TASK] Size before filtering by student ID: {df.count()}")
    print(f"[ORGANIZING TASK] Size after filtering by student ID: {filtered_df.count()}")
    filtered_df = filtered_df.select("id", "student_id", "counselor_id", "timestamp", "year", "month", "day", "hour")

    return filtered_df

def insert_to_table(filtered_df, conn):
    pdf = filtered_df.toPandas()

    try:
        conn.execute("""
            CREATE TABLE IF NOT EXISTS expl_meeting_requests (
                id VARCHAR PRIMARY KEY,
                student_id VARCHAR,
                counselor_id VARCHAR,
                timestamp TIMESTAMP,
                year INTEGER,
                month INTEGER,
                day INTEGER,
                hour INTEGER
            );
        """)
    except Exception as e:
        print("[ORGANIZING TASK] Failed to create table:", e)

    conn.execute('DELETE FROM expl_meeting_requests;')
    conn.execute('INSERT INTO expl_meeting_requests SELECT * FROM pdf;')
    print("[ORGANIZING TASK] After inserting:", conn.execute("SELECT COUNT(*) FROM expl_meeting_requests;").fetchone())

if __name__ == '__main__':
    spark = init_spark()
    conn = duckdb.connect(db_path)
    df = read_data(spark)
    filtered_df = generate_db_table(spark, df, conn)
    insert_to_table(filtered_df, conn)
    spark.sparkContext.stop()
    conn.close()
    print(f'[ORGANIZING TASK] Creating expl_meeting_requests completed')